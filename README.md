# Gemma 3 4Bの日本語VLMとしての評価とSFT

本リポジトリでは、Googleが提供するマルチモーダル指示チューニング済みモデル [`google/gemma-3-4b-it`](https://huggingface.co/google/gemma-3-4b-it) を用いた日本語画像質問応答（VQA）の評価に取り組んだ。

---

## 実施したこと

- `google/gemma-3-4b-it` を Colab上で動作させ、画像 + テキストのマルチモーダル推論を実装
- 日本語VQAベンチマーク：
  - **JA-VLM-Bench-In-the-Wild**
  - **JA-VG-VQA-500**
  に対して推論と評価を実行
- 評価指標として **ROUGE-L** を使用し、自動評価スクリプトを実装

---

## 現状・できなかったこと

- 使用した2つのベンチマーク（JA-VLM-Bench-In-the-Wild、JA-VG-VQA-500）のうち、特に `SakanaAI/JA-VG-VQA-500` においては、全データを使用した際に GPUメモリ使用量が上限に達してしまい、十分な対処ができなかった。
- `Heron-Bench` の評価には GPT-4 APIを使用する必要があり、APIコストや準備に関する検討を行ったが、予算や時間の都合により今回は実施を見送った。
- Vision-Language Modelのファインチューニングを試みたが、マルチモーダルモデルに対応する実装や学習設定が難しく、動作確認や精度向上に至るまでの実装が完了しなかった。

---

## ファイル構成（主要）

- `LLMATCH_kadai_Gemma3.ipynb`: 推論・評価処理を記述したノートブック
- `README.md`: このファイル

---

## 今後の予定・展望

- `SakanaAI/JA-VG-VQA-500` において発生した GPUメモリ制限の問題に対して、全データを用いたベンチマーク評価を安定して実施する方法について検討。
- `Heron-Bench` の評価について、GPT-4 APIの使用にかかるコスト試算やAPI利用体制の構築を進めた上で、実際の評価スクリプトの実装および自動評価フローの整備について検討。
- Vision-Language Modelのファインチューニングに関して、LoRAなどの軽量なファインチューニング手法を引き続き調査・検証し、マルチモーダル入力への対応を踏まえた学習・評価パイプラインの構築を検討。


